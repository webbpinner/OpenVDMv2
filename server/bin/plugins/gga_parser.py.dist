# =================================================================================== #
#
#         FILE:  gga_parser.py
#
#        USAGE:  gga_parser.py [-h] [-c] <dataFile>
#
#  DESCRIPTION:  Parse the supplied NMEA-formtted GGA file (w/ SCS formatted timestamp)
#                and return the json-formatted string used by OpenVDM as part of it's
#                Data dashboard.
#
#      OPTIONS:  [-h] Return the help message.
#                [-c] Use CSVkit to clean the datafile prior to processing
#                <dataFile> Full or relative path of the data file to process.
#
# REQUIREMENTS:  python2.7, Python Modules: sys, os, argparse, json, pandas
#
#         BUGS:
#        NOTES:
#       AUTHOR:  Webb Pinner
#      COMPANY:  Capable Solutions
#      VERSION:  1.0
#      CREATED:  2016-08-29
#     REVISION:  2016-10-30
#
# LICENSE INFO:  Open Vessel Data Management v2.2 (OpenVDMv2)
#                Copyright (C) 2017 OceanDataRat.org
#
#        NOTES:  Requires Pandas v0.18 or higher
#
#    This program is free software: you can redistribute it and/or modify it under the
#    terms of the GNU General Public License as published by the Free Software
#    Foundation, either version 3 of the License, or (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful, but WITHOUT ANY
#    WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A
#    PARTICULAR PURPOSE.  See the GNU General Public License for more details.
#
#    You should have received a copy of the GNU General Public License #    along with
#    this program.  If not, see <http://www.gnu.org/licenses/>.
#
# =================================================================================== #
import os
import sys
import csv
import json
import shutil
import pandas as pd
import numpy as np
import argparse
import tempfile
import logging
from datetime import datetime
from geopy import Point
from geopy.distance import great_circle

from os.path import dirname, realpath
sys.path.append(dirname(dirname(dirname(dirname(realpath(__file__))))))

from server.lib.openvdm_plugin import OpenVDMPluginCSV, csvCleanup

RAW_COLUMNS = ['date','time','hdr','gps_time','latitude','NS','longitude','EW','fix_quality','num_satellites','hdop','altitude','altitude_m','height_wgs84','height_wgs84_m','last_update','dgps_station_checksum']
PROC_COLUMNS = ['date_time','latitude','longitude','num_satellites','hdop','altitude','height_wgs84']
CROP_COLUMNS = ['date_time','latitude','longitude']

ROUNDING = {
    'latitude': 8,
    'longitude': 8
}

MAX_VELOCITY = 13.8 #Max speed of vessel (mph)
MAX_DELTA_T = pd.Timedelta('10 seconds')


class GGAPlugin(OpenVDMPluginCSV):

    def __int__(self, raw_cols, proc_cols, crop_cols, csvkit=False, start_ts=None, stop_ts=None):
        self.start = start_ts
        self.stop = stop_ts
        logging.debug("start: {}".format(self.start))
        super().__init__(raw_cols=raw_cols, proc_cols=proc_cols, crop_cols=crop_cols, csvkit=csvkit)


    def _hemisphere_correction(self, lat_lon, hemisphere):
        if hemisphere == 'W' or hemisphere == "S":
            return lat_lon * -1.0

        return lat_lon

    def __str__(self):
        return "start: {}, stop: {}".format(self.start, self.stop)

    def process_file(self, filePath):

        errors = 0

        if self.csvkit:
            tmpdir = tempfile.mkdtemp()    
            shutil.copy(filePath, tmpdir)
            (errors, outfile) = csvCleanup(os.path.join(tmpdir, os.path.basename(filePath)))
            logging.debug('Errors: {}'.format(errors))
            filePath = outfile
          
        # raw_into_df = {'date_time':[],'latitude':[],'longitude':[],'num_satellites':[],'hdop':[],'altitude':[],'height_wgs84':[]}
        raw_into_df = { value: [] for key, value in enumerate(self.proc_cols) }

        with open(filePath, 'r') as csvfile:
            reader = csv.DictReader( csvfile, self.raw_cols)

            for line in reader:

                try:
                    date_time = ' '.join([line['date'], line['time']])
                    longitude = (self._hemisphere_correction(float(line['longitude'][:3]) + float(line['longitude'][3:])/60, line['EW']))
                    latitude = (self._hemisphere_correction(float(line['latitude'][:2]) + float(line['latitude'][2:])/60, line['NS']))
                    num_satellites = int(line['num_satellites'])
                    hdop = float(line['hdop'])
                    altitude = float(line['altitude']) if line['altitude'] != '' else 0.0
                    height_wgs84 = float(line['height_wgs84']) if line['height_wgs84'] != '' else 0.0

                except Exception as err:
                    logging.warning('Parsing error: {}'.format(line))
                    logging.debug(str(err))
                    errors += 1

                else:
                    raw_into_df['date_time'].append(date_time)
                    raw_into_df['longitude'].append(longitude)
                    raw_into_df['latitude'].append(latitude)
                    raw_into_df['num_satellites'].append(num_satellites)
                    raw_into_df['hdop'].append(hdop)
                    raw_into_df['altitude'].append(altitude)
                    raw_into_df['height_wgs84'].append(height_wgs84)

        logging.info("Finished parsing data file")
        if self.csvkit:
            shutil.rmtree(tmpdir)

        # If no data ingested from file, quit
        if len(raw_into_df['date_time']) == 0:
            logging.warning("dataframe is empty, quitting")
            return None

        # Build DataFrame
        logging.debug("building dataframe")
        df_proc = pd.DataFrame(raw_into_df)

        # Convert Date/time column to datetime objects
        logging.debug("converting data_time to datetime datatype")
        df_proc['date_time'] = pd.to_datetime(df_proc['date_time'], infer_datetime_format=True)

        # Optionally crop data by start/stop times
        if self.start or self.stop_ts:
            logging.debug("cropping data")

        if self.start is not None:
            logging.debug("start: {}".format(self.start))
            df_proc = df_proc[(df_proc['date_time'] >= self.start)]

        if self.stop is not None:
            logging.debug("stop: {}".format(self.stop))
            df_proc = df_proc[(df_proc['date_time'] <= self.stop)]

        # If the crop operation emptied the dataframe, quit
        if df_proc.shape[0] == 0:
            logging.warning("cropped dataframe is now empty, quitting")
            return None

        # set index
        df_proc = df_proc.set_index('date_time')

        # Calculate deltaT column
        df_proc = df_proc.join(df_proc['date_time'].diff().to_frame(name='deltaT'))

        # Calculate distance column
        df_proc['point'] = df_proc.apply(lambda row: Point(latitude=row['latitude'], longitude=row['longitude']), axis=1)
        df_proc['point_next'] = df_proc['point'].shift(1)
        df_proc.loc[df_proc['point_next'].isna(), 'point_next'] = None
        
        df_proc['distance'] = df_proc.apply(lambda row: great_circle(row['point'], row['point_next']).nm if row['point_next'] is not None else float('nan'), axis=1)
        df_proc = df_proc.drop('point_next', axis=1)
        df_proc = df_proc.drop('point', axis=1)

        # Calculate velocity column
        df_proc['velocity'] = df_proc['distance'] / (df_proc.deltaT.dt.total_seconds() / 3600)

        logging.debug("calculating stats")
        self.add_stat({'statName': 'Row Validity', 'statType': 'rowValidity', 'statData': [len(df_proc), errors]})
        self.add_stat({'statName': 'Geographic Bounds', 'statUnit': 'ddeg', 'statType': 'geoBounds', 'statData': [round(df_proc.latitude.max(),3),round(df_proc.longitude.max(),3),round(df_proc.latitude.min(),3),round(df_proc.longitude.min(),3)]})
        self.add_stat({'statName': 'Velocity Bounds', 'statUnit': 'kts', 'statType': 'bounds', 'statData': [round(df_proc.velocity.min(),3), 999999.999 if np.isinf(df_proc.velocity.max()) else round(df_proc.velocity.max(),3)]})
        self.add_stat({'statName': 'Velocity Validity', 'statType': 'valueValidity', 'statData': [len(df_proc[(df_proc['velocity'] <= MAX_VELOCITY)]),len(df_proc[(df_proc['velocity'] > MAX_VELOCITY)])]})
        self.add_stat({'statName': 'Distance Traveled', 'statUnit': 'nm', 'statType': 'totalValue', 'statData': [round(df_proc.distance.sum(axis=0),3)]})
        self.add_stat({'statName': 'Temporal Bounds', 'statUnit': 'seconds', 'statType': 'timeBounds', 'statData': [df_proc.date_time.min().strftime('%s'), df_proc.date_time.max().strftime('%s')]})
        self.add_stat({"statName": 'Delta-T Bounds', 'statUnit': 'seconds', 'statType': 'bounds','statData': [round(df_proc.deltaT.min().total_seconds(),3), round(df_proc.deltaT.max().total_seconds(),3)]})
        self.add_stat({'statName': 'DeltaT Validity', 'statType': 'valueValidity', 'statData': [len(df_proc[(df_proc['deltaT'] <= MAX_DELTA_T)]),len(df_proc[(df_proc['deltaT'] > MAX_DELTA_T)])]})
        self.add_stat({'statName': 'Number of Satellites', 'statUnit': 'sats', 'statType': 'bounds', 'statData': [round(df_proc.num_satellites.min(),3), round(df_proc.num_satellites.max(),3)]})
        self.add_stat({'statName': 'Horizontal Degree of Precision', 'statUnit': '', 'statType': 'bounds', 'statData': [round(df_proc.hdop.min(),3), round(df_proc.hdop.max(),3)]})
        self.add_stat({'statName': 'Altitude', 'statUnit': 'm', 'statType': 'bounds', 'statData': [round(df_proc.altitude.min(),3), round(df_proc.altitude.max(),3)]})
        self.add_stat({'statName': 'Height WGS84', 'statUnit': 'm', 'statType': 'bounds', 'statData': [round(df_proc.height_wgs84.min(),3), round(df_proc.height_wgs84.max(),3)]})

        logging.debug("calculating quality tests")
        # % of bad rows in datafile
        error_rate = errors / len(df_proc)
        if error_rate > .25:
            self.add_quality_test({"testName": "Rows", "results": "Failed"})
        elif error_rate > .10:
            self.add_quality_test({"testName": "Rows", "results": "Warning"})
        else:
            self.add_quality_test({"testName": "Rows", "results": "Passed"})

        # % of time gaps in data
        error_rate = len(df_proc[(df_proc['deltaT'] > MAX_DELTA_T)]) / len(df_proc)
        if error_rate > .25:
            self.add_quality_test({"testName": "DeltaT", "results": "Failed"})
        elif error_rate > .10:
            self.add_quality_test({"testName": "DeltaT", "results": "Warning"})
        else:
            self.add_quality_test({"testName": "DeltaT", "results": "Passed"})

        # % of bad velocities in data
        error_rate = len(df_proc[(df_proc['velocity'] > MAX_VELOCITY)]) / len(df_proc)
        if error_rate > .25:
            self.add_quality_test({"testName": "DeltaT", "results": "Failed"})
        elif error_rate > .10:
            self.add_quality_test({"testName": "DeltaT", "results": "Warning"})
        else:
            self.add_quality_test({"testName": "DeltaT", "results": "Passed"})

        # set index
        # df_proc = df_proc.set_index('date_time')

        # resample data
        logging.debug("resampling data")
        df_proc = self.resample_data(df_proc)

        # round data
        logging.debug("rounding data")
        df_proc = self.round_data(df, ROUNDING)

        # split data where there are gaps
        logging.debug("building visualization data")
        events = np.split(proc_df, np.where(np.isnan(proc_df['latitude']))[0])

        # removing NaN entries
        events = [ev[~np.isnan(ev.latitude)] for ev in events if not isinstance(ev, np.ndarray)]

        # removing empty DataFrames
        events = [ev for ev in events if not ev.empty]

        visualizerDataObj = {
            'type':'FeatureCollection',
            'features': []
        }

        for ev in events:
            logging.debug("EV: {}".format(ev))
            feature = {
                'type':'Feature',
                'geometry':{
                    'type':'LineString',
                    'coordinates':json.loads(ev[['longitude','latitude']].to_json(orient='values'))
                },
                'properties': {
                    'coordTimes': json.loads(ev['date_time'].to_json(orient='values')),
                    'name': filePath
                }
            }
            #print(feature)
            visualizerDataObj['features'].append(feature)

        self.add_visualization_data(visualizerDataObj)


# -------------------------------------------------------------------------------------
# Required python code for running the script as a stand-alone utility
# -------------------------------------------------------------------------------------
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Parse NMEA GGA data')
    parser.add_argument('-v', '--verbosity', dest='verbosity',
                        default=0, action='count',
                        help='Increase output verbosity')
    parser.add_argument('-c', '--csvkit', action='store_true', default=False,
                        help='clean datafile using CSVKit')
    parser.add_argument('--startTS', default=None,
                        type=lambda s: datetime.strptime(s, '%Y-%m-%dT%H:%M:%S.%fZ'),
                        help=' crop start timestamp (iso8601)')
    parser.add_argument('--stopTS', default=None,
                        type=lambda s: datetime.strptime(s, '%Y-%m-%dT%H:%M:%S.%fZ'),
                        help=' crop stop timestamp (iso8601)')
    parser.add_argument('dataFile', metavar='dataFile',
                        help='the raw data file to process')

    parsed_args = parser.parse_args()

    ############################
    # Set up logging before we do any other argument parsing (so that we
    # can log problems with argument parsing).
    
    LOGGING_FORMAT = '%(asctime)-15s %(levelname)s - %(message)s'
    logging.basicConfig(format=LOGGING_FORMAT)

    LOG_LEVELS = {0: logging.WARNING, 1: logging.INFO, 2: logging.DEBUG}
    parsed_args.verbosity = min(parsed_args.verbosity, max(LOG_LEVELS))
    logging.getLogger().setLevel(LOG_LEVELS[parsed_args.verbosity])

    ovdm_plugin = GGAPlugin(RAW_COLUMNS, PROC_COLUMNS, CROP_COLUMNS, csvkit=parsed_args.csvkit)
    logging.debug(str(ovdm_plugin))

    if not os.path.isfile(parsed_args.dataFile):
        logging.error('Data file not found: {}'.format(parsed_args.dataFile))
        sys.exit(1)

    try:
        ovdm_plugin.process_file(parsed_args.dataFile)
        print(json.dumps(ovdm_plugin.get_plugin_data()))
    except Exception as err:
        logging.error(str(err))
        sys.exit(1)
