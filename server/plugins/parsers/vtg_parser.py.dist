# =================================================================================== #
#
#         FILE:  vtg_parser.py
#
#        USAGE:  vtg_parser.py [-h] [-v+] [--timeFormat] [--startDT] [--stopDT] <dataFile>
#
#  DESCRIPTION:  Parse the supplied NMEA-formtted VTG file and return the json-
#                formatted string used by OpenVDM as part of it's Data dashboard.
#
#      OPTIONS:  [-h] Return the help message.
#                [-v] Increase verbosity (default: warning)
#                [--timeFormat] date/time format to use when parsing datafile, default
#                               yyyy-mm-ddTHH:MM:SS.sssZ
#                [--startTS] optional start crop time (strptime format)
#                [--stopTS] optional stop crop time (strptime format)
#                <dataFile> Full or relative path of the data file to process.
#
# REQUIREMENTS:  Python3.8
#                Python Modules:
#                    numpy==1.19.5
#                    pandas==1.2.0
#                    PyYAML==5.3.1
#                    requests==2.25.1
#
#         BUGS:
#        NOTES:
#       AUTHOR:  Webb Pinner
#      COMPANY:  Capable Solutions
#      VERSION:  2.5
#      CREATED:  2016-08-29
#     REVISION:  2021-01-16
#
# LICENSE INFO:  Open Vessel Data Management v2.5 (OpenVDMv2)
#                Copyright (C) 2021 OceanDataRat.org
#
#    This program is free software: you can redistribute it and/or modify it under the
#    terms of the GNU General Public License as published by the Free Software
#    Foundation, either version 3 of the License, or (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful, but WITHOUT ANY
#    WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A
#    PARTICULAR PURPOSE.  See the GNU General Public License for more details.
#
#    You should have received a copy of the GNU General Public License #    along with
#    this program.  If not, see <http://www.gnu.org/licenses/>.
#
# =================================================================================== #

import os
import sys
import csv
import json
import pandas as pd
import numpy as np
import argparse
import logging
from copy import deepcopy
from datetime import datetime

from os.path import dirname, realpath
sys.path.append(dirname(dirname(dirname(dirname(realpath(__file__))))))

from server.lib.openvdm_plugin import OpenVDMCSVParser
from server.lib.openvdm import OpenVDM_API

# RAW_COLS = ['date_time','hdr','cog_t','T','cog_m','M','sog_kts','N','sog_kph','K','checksum'] # OpenRVDAS style
RAW_COLS = ['date', 'time','hdr','cog_t','T','cog_m','M','sog_kts','N','sog_kph','K','checksum'] # SCS style
PROC_COLS = ['date_time','cog_t','cog_m','sog_kts','sog_kph']

ROUNDING = {
    'cog_t': 3,
    'cog_m': 3,
    'sog_kts': 3,
    'sog_kph': 3
}

MIN_COG = 0
MAX_COG = 360

MAX_SOG_KTS = 25
MAX_SOG_KPH = MAX_SOG_KTS * 1.852

MAX_DELTA_T = pd.Timedelta('10 seconds')

# DEFAULT_TIME_FORMAT = "%Y-%m-%dT%H:%M:%S.%fZ" # ISO8601 Format, OpenRVDAS style
DEFAULT_TIME_FORMAT = "%m/%d/%Y %H:%M:%S.%f" # SCS style


class VTGParser(OpenVDMCSVParser):

    def __init__(self, start_dt=None, stop_dt=None, time_format=DEFAULT_TIME_FORMAT, use_openvdm_api=True):
        super().__init__(start_dt=start_dt, stop_dt=stop_dt)
        self.raw_cols = RAW_COLS
        self.proc_cols = PROC_COLS
        self.time_format = time_format
        self.openvdm = OpenVDM_API() if use_openvdm_api else None


    def process_file(self, filePath):

        raw_into_df = { value: [] for key, value in enumerate(self.proc_cols) }

        logging.debug("Parsing data file...")
        errors = []
        try:
            with open(filePath, 'r') as csvfile:
                reader = csv.DictReader(csvfile, self.raw_cols)

                for lineno, line in enumerate(reader):

                    try:
                        # date_time = line['date_time'] # OpenRVDAS style
                        date_time = ' '.join([line['date'], line['time']]) # SCS style
                        
                        cog_t = np.nan if line['cog_t'] == '' else float(line['cog_t'])
                        cog_m = np.nan if line['cog_m'] == '' else float(line['cog_m'])
                        sog_kts = np.nan if line['sog_kts'] == '' else float(line['sog_kts'])
                        sog_kph = np.nan if line['sog_kph'] == '' else float(line['sog_kph'])

                    except Exception as err:
                        errors.append(lineno)
                        logging.warning("Parsing error encountered on line {}".format(lineno))
                        logging.debug(line)
                        logging.debug(str(err))

                    else:
                        raw_into_df['date_time'].append(date_time)
                        raw_into_df['cog_t'].append(cog_t)
                        raw_into_df['cog_m'].append(cog_m)
                        raw_into_df['sog_kts'].append(sog_kts)
                        raw_into_df['sog_kph'].append(sog_kph)

        except Exception as err:
            logging.error("Problem accessing input file: {}".format(filePath))
            logging.error(str(err))
            return None

        logging.debug("Finished parsing data file")
        
        # If no data ingested from file, quit
        if len(raw_into_df['date_time']) == 0:
            logging.warning("Dataframe is empty... quitting")
            return None

        # Build DataFrame
        logging.debug("Building dataframe from parsed data...")
        df_proc = pd.DataFrame(raw_into_df)

        # Convert Date/time column to datetime objects
        logging.debug("Converting data_time to datetime datatype...")
        
        df_proc['date_time'] = pd.to_datetime(df_proc['date_time'], format=self.time_format)

        # Optionally crop data by start/stop times
        if self.start_dt or self.stop_dt:
            logging.debug("Cropping data...")

            df_proc = self.crop_data(df_proc)

        # If the crop operation emptied the dataframe, quit
        if df_proc.shape[0] == 0:
            logging.warning("Cropped dataframe is empty... quitting")
            return None

        # Calculate deltaT column
        logging.debug('Building deltaT column...')
        df_proc = df_proc.join(df_proc['date_time'].diff().to_frame(name='deltaT'))

        logging.debug("Tabulating statistics...")
        self.add_row_validity_stat([len(df_proc), len(errors)])
        self.add_time_bounds_stat([df_proc['date_time'].min(), df_proc['date_time'].max()])
        self.add_bounds_stat([round(df_proc['deltaT'].min().total_seconds(),3), round(df_proc['deltaT'].max().total_seconds(),3)], 'DeltaT Bounds', 'seconds')
        self.add_value_validity_stat([len(df_proc[(df_proc['deltaT'] <= MAX_DELTA_T)]),len(df_proc[(df_proc['deltaT'] > MAX_DELTA_T)])], 'DeltaT Validity')
        self.add_bounds_stat([round(df_proc['cog_t'].min(),3), round(df_proc['cog_t'].max(),3)], "COG True Bounds", "deg")
        self.add_value_validity_stat([len(df_proc[(df_proc['cog_t'] >= MIN_COG) & (df_proc['cog_t'] <= MAX_COG)]),len(df_proc[(df_proc['cog_t'] < MIN_COG) & (df_proc['cog_t'] > MAX_COG)])], "COG True Validity")
        self.add_bounds_stat([round(df_proc['cog_m'].min(),3), round(df_proc['cog_m'].max(),3)], "COG Magnetic Bounds", "deg")
        self.add_value_validity_stat([len(df_proc[(df_proc['cog_t'] >= MIN_COG) & (df_proc['cog_t'] <= MAX_COG)]),len(df_proc[(df_proc['cog_t'] < MIN_COG) & (df_proc['cog_t'] > MAX_COG)])], "COG Magnetic Validity")
        self.add_bounds_stat([round(df_proc['sog_kts'].min(),3), round(df_proc['sog_kts'].max(),3)], 'SOG Knots Bounds', 'deg')
        self.add_value_validity_stat([len(df_proc[(df_proc['sog_kts'] <= MAX_SOG_KTS)]),len(df_proc[(df_proc['sog_kts'] > MAX_SOG_KTS)])], "SOG Knots Validity")
        self.add_bounds_stat([round(df_proc['sog_kph'].min(),3), round(df_proc['sog_kph'].max(),3)], 'SOG KPH Bounds', 'deg')
        self.add_value_validity_stat([len(df_proc[(df_proc['sog_kph'] <= MAX_SOG_KPH)]),len(df_proc[(df_proc['sog_kph'] > MAX_SOG_KPH)])], "SOG KPH Validity")

    
        logging.debug("Running quality tests...")
        # % of bad rows in datafile
        error_rate = len(errors) / (len(df_proc) + len(errors))
        if error_rate > .25:
            self.add_quality_test_failed("Rows")
        elif error_rate > .10:
            self.add_quality_test_warning("Rows")
        else:
            self.add_quality_test_passed("Rows")

        # % of time gaps in data
        error_rate = len(df_proc[(df_proc['deltaT'] > MAX_DELTA_T)]) / len(df_proc)
        if error_rate > .25:
            self.add_quality_test_failed("DeltaT")
        elif error_rate > .10:
            self.add_quality_test_warning("DeltaT")
        else:
            self.add_quality_test_passed("DeltaT")

        error_rate = len(df_proc[(df_proc['sog_kts'] > MAX_SOG_KTS)]) / len(df_proc)
        if error_rate > .25:
            self.add_quality_test_failed("SOG KTS")
        elif error_rate > .10:
            self.add_quality_test_warning("SOG KTS")
        else:
            self.add_quality_test_passed("SOG KTS")

        error_rate = len(df_proc[(df_proc['sog_kph'] > MAX_SOG_KPH)]) / len(df_proc)
        if error_rate > .25:
            self.add_quality_test_failed("SOG KPH")
        elif error_rate > .10:
            self.add_quality_test_warning("SOG KPH")
        else:
            self.add_quality_test_passed("SOG KPH")

        error_rate = len(df_proc[(df_proc['cog_t'] < MIN_COG) & (df_proc['cog_t'] > MAX_COG)]) / len(df_proc)
        if error_rate > .25:
            self.add_quality_test_failed("COG True")
        elif error_rate > .10:
            self.add_quality_test_warning("COG True")
        else:
            self.add_quality_test_passed("COG True")

        error_rate = len(df_proc[(df_proc['cog_m'] < MIN_COG) & (df_proc['cog_m'] > MAX_COG)]) / len(df_proc)
        if error_rate > .25:
            self.add_quality_test_failed("COG Magnetic")
        elif error_rate > .10:
            self.add_quality_test_warning("COG Magnetic")
        else:
            self.add_quality_test_passed("COG Magnetic")

        # set index
        logging.debug('Setting index...')
        df_proc = df_proc.set_index('date_time')

        # resample data
        logging.debug("Resampling data...")
        df_proc = self.resample_data(df_proc)

        # round data
        logging.debug("Rounding data: {}".format(ROUNDING))
        df_proc = self.round_data(df_proc, ROUNDING)

        # split data where there are gaps
        logging.debug("Building visualization data...")

        visualizerDataObj = {'data':[], 'unit':'', 'label':''}
        visualizerDataObj['data'] = json.loads(df_proc[['date_time','cog_t']].to_json(orient='values'))
        visualizerDataObj['unit'] = 'deg'
        visualizerDataObj['label'] = 'COG, True'
        self.add_visualization_data(deepcopy(visualizerDataObj))

        visualizerDataObj['data'] = json.loads(df_proc[['date_time','cog_m']].to_json(orient='values'))
        visualizerDataObj['unit'] = 'deg'
        visualizerDataObj['label'] = 'COG, Magnetic'
        self.add_visualization_data(deepcopy(visualizerDataObj))
        
        visualizerDataObj['data'] = json.loads(df_proc[['date_time','sog_kts']].to_json(orient='values'))
        visualizerDataObj['unit'] = 'kts'
        visualizerDataObj['label'] = 'SOG, kts'
        self.add_visualization_data(deepcopy(visualizerDataObj))

        visualizerDataObj['data'] = json.loads(df_proc[['date_time','sog_kph']].to_json(orient='values'))
        visualizerDataObj['unit'] = 'kph'
        visualizerDataObj['label'] = 'SOG, kph'
        self.add_visualization_data(deepcopy(visualizerDataObj))

        # send message about errors encountered to OpenVDM
        if self.openvdm is not None and len(errors) > 0:
            self.openvdm.sendMsg('Parsing Error', 'Error(s) parsing datafile {} on row(s): {}'.format(filePath, ', '.join([str(error) for error in errors])))


# -------------------------------------------------------------------------------------
# Required python code for running the script as a stand-alone utility
# -------------------------------------------------------------------------------------
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Parse NMEA VTG data')
    parser.add_argument('-v', '--verbosity', dest='verbosity',
                        default=0, action='count',
                        help='Increase output verbosity')
    parser.add_argument('--timeFormat', default=DEFAULT_TIME_FORMAT,
                        help='timestamp format, default: %(default)')
    parser.add_argument('--startDT', default=None,
                        type=lambda s: datetime.strptime(s, '%Y-%m-%dT%H:%M:%S.%fZ'),
                        help=' crop start timestamp (iso8601)')
    parser.add_argument('--stopDT', default=None,
                        type=lambda s: datetime.strptime(s, '%Y-%m-%dT%H:%M:%S.%fZ'),
                        help=' crop stop timestamp (iso8601)')
    parser.add_argument('dataFile', metavar='dataFile',
                        help='the raw data file to process')

    parsed_args = parser.parse_args()

    ############################
    # Set up logging before we do any other argument parsing (so that we
    # can log problems with argument parsing).
    
    LOGGING_FORMAT = '%(asctime)-15s %(levelname)s - %(message)s'
    logging.basicConfig(format=LOGGING_FORMAT)

    LOG_LEVELS = {0: logging.WARNING, 1: logging.INFO, 2: logging.DEBUG}
    parsed_args.verbosity = min(parsed_args.verbosity, max(LOG_LEVELS))
    logging.getLogger().setLevel(LOG_LEVELS[parsed_args.verbosity])

    ovdm_parser = VTGParser(start_dt=parsed_args.startDT, stop_dt=parsed_args.stopDT, time_format=parsed_args.timeFormat)

    try:
        logging.info("Processing file: {}".format(parsed_args.dataFile))
        ovdm_parser.process_file(parsed_args.dataFile)
        print(ovdm_parser.toJSON())
        logging.info("Done!")
    except Exception as err:
        logging.error(str(err))
        raise err
        sys.exit(1)
