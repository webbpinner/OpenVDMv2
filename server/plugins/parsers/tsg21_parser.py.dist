# =================================================================================== #
#
#         FILE:  tsg21_parser.py
#
#        USAGE:  tsg21_parser.py [-h] [-v+] [--timeFormat] [--startDT] [--stopDT] <dataFile>
#
#  DESCRIPTION:  Parse the supplied SBE21 TSG data and return the json-formatted
#                string used by OpenVDM as part of it's Data dashboard.
#
#      OPTIONS:  [-h] Return the help message.
#                [-v] Increase verbosity (default: warning)
#                [--timeFormat] date/time format to use when parsing datafile, default
#                               yyyy-mm-ddTHH:MM:SS.sssZ
#                [--startTS] optional start crop time (strptime format)
#                [--stopTS] optional stop crop time (strptime format)
#                <dataFile> Full or relative path of the data file to process.
#
# REQUIREMENTS:  Python3.8
#                Python Modules:
#                    numpy==1.19.5
#                    pandas==1.2.0
#                    PyYAML==5.3.1
#                    requests==2.25.1
#
#         BUGS:
#        NOTES:
#       AUTHOR:  Webb Pinner
#      COMPANY:  Capable Solutions
#      VERSION:  2.5
#      CREATED:  2016-08-29
#     REVISION:  2021-01-17
#
# LICENSE INFO:  Open Vessel Data Management v2.5 (OpenVDMv2)
#                Copyright (C) 2021 OceanDataRat.org
#
#    This program is free software: you can redistribute it and/or modify it under the
#    terms of the GNU General Public License as published by the Free Software
#    Foundation, either version 3 of the License, or (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful, but WITHOUT ANY
#    WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A
#    PARTICULAR PURPOSE.  See the GNU General Public License for more details.
#
#    You should have received a copy of the GNU General Public License #    along with
#    this program.  If not, see <http://www.gnu.org/licenses/>.
#
# =================================================================================== #

import os
import sys
import csv
import json
import pandas as pd
import numpy as np
import argparse
import logging
from copy import deepcopy
from datetime import datetime

from os.path import dirname, realpath
sys.path.append(dirname(dirname(dirname(dirname(realpath(__file__))))))

from server.lib.openvdm_plugin import OpenVDMCSVParser
from server.lib.openvdm import OpenVDM_API

RAW_COLS = ['date_time','data'] # OpenRVDAS style
# RAW_COLS = ['date','time','data'] # SCS style
PROC_COLUMNS = ['date_time','conductivity','water_temp_1','salinity','water_temp_2', 'fluorescence','sound_velocity']

ROUNDING = {
    'conductivity': 5,
    'water_temp_1': 4,
    'salinity': 4,
    'water_temp_2': 4,
    'fluorescence': 5,
    'sound_velocity': 5
}

MIN_DEPTH = 0

MAX_DELTA_T = pd.Timedelta('10 seconds')

DEFAULT_TIME_FORMAT = "%Y-%m-%dT%H:%M:%S.%fZ" # ISO8601 Format, OpenRVDAS style
# DEFAULT_TIME_FORMAT = "%m/%d/%Y %H:%M:%S.%f" # SCS style


class SBE21TSGParser(OpenVDMCSVParser):

    def __init__(self, start_dt=None, stop_dt=None, time_format=DEFAULT_TIME_FORMAT, use_openvdm_api=True):
        super().__init__(start_dt=start_dt, stop_dt=stop_dt)
        self.raw_cols = RAW_COLS
        self.proc_cols = PROC_COLS
        self.time_format = time_format
        self.openvdm = OpenVDM_API() if use_openvdm_api else None


    def process_file(self, filePath):

        raw_into_df = { value: [] for key, value in enumerate(self.proc_cols) }

        logging.debug("Parsing data file...")
        errors = []
        try:
            with open(filePath, 'r') as csvfile:
                reader = csv.DictReader(csvfile, self.raw_cols)

                for lineno, line in enumerate(reader):

                    try:
                        date_time = line['date_time'] # OpenRVDAS style
                        #date_time = ' '.join([line['date'], line['time']]) # SCS style

                        data_cont,data_temp_1,data_sal,data_temp_2,data_fluoro,data_sv =  line['data'].split()

                        conductivity = float(data_cont)
                        water_temp_1 = float(data_temp_1)
                        salinity = float(data_sal)
                        water_temp_2 = float(data_temp_2)
                        fluorescence = float(data_fluoro)
                        sound_velocity = float(data_sv)                        

                    except Exception as err:
                        errors.append(lineno)
                        logging.warning("Parsing error encountered on line {}".format(lineno))
                        logging.debug(line)
                        logging.debug(str(err))

                    else:
                        raw_into_df['date_time'].append(date_time)
                        raw_into_df['conductivity'].append(conductivity)
                        raw_into_df['water_temp_1'].append(water_temp_1)
                        raw_into_df['salinity'].append(salinity)
                        raw_into_df['water_temp_2'].append(water_temp_2)
                        raw_into_df['fluorescence'].append(fluorescence)
                        raw_into_df['sound_velocity'].append(sound_velocity)

        except Exception as err:
            logging.error("Problem accessing input file: {}".format(filePath))
            logging.error(str(err))
            return None

        logging.debug("Finished parsing data file")
        
        # If no data ingested from file, quit
        if len(raw_into_df['date_time']) == 0:
            logging.warning("Dataframe is empty... quitting")
            return None

        # Build DataFrame
        logging.debug("Building dataframe from parsed data...")
        df_proc = pd.DataFrame(raw_into_df)

        # Convert Date/time column to datetime objects
        logging.debug("Converting data_time to datetime datatype...")
        
        df_proc['date_time'] = pd.to_datetime(df_proc['date_time'], format=self.time_format)

        # Optionally crop data by start/stop times
        if self.start_dt or self.stop_dt:
            logging.debug("Cropping data...")

            df_proc = self.crop_data(df_proc)

        # If the crop operation emptied the dataframe, quit
        if df_proc.shape[0] == 0:
            logging.warning("Cropped dataframe is empty... quitting")
            return None

        # Calculate deltaT column
        logging.debug('Building deltaT column...')
        df_proc = df_proc.join(df_proc['date_time'].diff().to_frame(name='deltaT'))

        logging.debug("Tabulating statistics...")
        self.add_row_validity_stat([len(df_proc), len(errors)])
        self.add_time_bounds_stat([df_proc['date_time'].min(), df_proc['date_time'].max()])
        self.add_bounds_stat([round(df_proc['deltaT'].min().total_seconds(),3), round(df_proc['deltaT'].max().total_seconds(),3)], 'DeltaT Bounds', 'seconds')
        self.add_value_validity_stat([len(df_proc[(df_proc['deltaT'] <= MAX_DELTA_T)]),len(df_proc[(df_proc['deltaT'] > MAX_DELTA_T)])], 'DeltaT Validity')
        self.add_bounds_stat([round(df_proc['conductivity'].min(),5), round(df_proc['conductivity'].max(),5)], 'Conductivity', 'S/m')
        self.add_bounds_stat([round(df_proc['water_temp_1'].min(),4), round(df_proc['water_temp_1'].max(),4)], 'Water Temp 1', 'C')
        self.add_bounds_stat([round(df_proc['salinity'].min(),4), round(df_proc['salinity'].max(),4)], 'Salinity', 'PSU')
        self.add_bounds_stat([round(df_proc['water_temp_2'].min(),4), round(df_proc['water_temp_2'].max(),4)], 'Water Temp 2', 'C')
        self.add_bounds_stat([round(df_proc['fluorescence'].min(),5), round(df_proc['fluorescence'].max(),5)], 'Fluorescence', 'mg/m^3')
        self.add_bounds_stat([round(df_proc['sound_velocity'].min(),5), round(df_proc['sound_velocity'].max(),5)], 'Sound Velocity', 'm/s')
        
        logging.debug("Running quality tests...")
        # % of bad rows in datafile
        error_rate = len(errors) / (len(df_proc) + len(errors))
        if error_rate > .25:
            self.add_quality_test_failed("Rows")
        elif error_rate > .10:
            self.add_quality_test_warning("Rows")
        else:
            self.add_quality_test_passed("Rows")

        # % of time gaps in data
        error_rate = len(df_proc[(df_proc['deltaT'] > MAX_DELTA_T)]) / len(df_proc)
        if error_rate > .25:
            self.add_quality_test_failed("DeltaT")
        elif error_rate > .10:
            self.add_quality_test_warning("DeltaT")
        else:
            self.add_quality_test_passed("DeltaT")

        # set index
        logging.debug('Setting index...')
        df_proc = df_proc.set_index('date_time')

        # resample data
        logging.debug("Resampling data...")
        df_proc = self.resample_data(df_proc)

        # round data
        logging.debug("Rounding data: {}".format(ROUNDING))
        df_proc = self.round_data(df_proc, ROUNDING)

        # split data where there are gaps
        logging.debug("Building visualization data...")

        visualizerDataObj = {'data':[], 'unit':'', 'label':''}
        visualizerDataObj['data'] = json.loads(df_proc[['date_time','water_temp_1']].to_json(orient='values'))
        visualizerDataObj['unit'] = 'C'
        visualizerDataObj['label'] = 'Water Temp 1'
        self.add_visualization_data(deepcopy(visualizerDataObj))

        visualizerDataObj['data'] = json.loads(df_proc[['date_time','conductivity']].to_json(orient='values'))
        visualizerDataObj['unit'] = 'S/m'
        visualizerDataObj['label'] = 'Conductivity'
        self.add_visualization_data(deepcopy(visualizerDataObj))

        visualizerDataObj['data'] = json.loads(df_proc[['date_time','salinity']].to_json(orient='values'))
        visualizerDataObj['unit'] = 'PSU'
        visualizerDataObj['label'] = 'Salinity'
        self.add_visualization_data(deepcopy(visualizerDataObj))

        visualizerDataObj['data'] = json.loads(df_proc[['date_time','water_temp_2']].to_json(orient='values'))
        visualizerDataObj['unit'] = 'C'
        visualizerDataObj['label'] = 'Water Temp 2'
        self.add_visualization_data(deepcopy(visualizerDataObj))

        visualizerDataObj['data'] = json.loads(df_proc[['date_time','fluorescence']].to_json(orient='values'))
        visualizerDataObj['unit'] = 'mg/m^3'
        visualizerDataObj['label'] = 'Fluorescence'
        self.add_visualization_data(deepcopy(visualizerDataObj))

        visualizerDataObj['data'] = json.loads(df_proc[['date_time','sound_velocity']].to_json(orient='values'))
        visualizerDataObj['unit'] = 'm/s'
        visualizerDataObj['label'] = 'Sound Velocity'
        self.add_visualization_data(deepcopy(visualizerDataObj))

        # send message about errors encountered to OpenVDM
        if self.openvdm is not None and len(errors) > 0:
            self.openvdm.sendMsg('Parsing Error', 'Error(s) parsing datafile {} on row(s): {}'.format(filePath, ', '.join([str(error) for error in errors])))


# -------------------------------------------------------------------------------------
# Required python code for running the script as a stand-alone utility
# -------------------------------------------------------------------------------------
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Parse SBE21 TSG sensor data')
    parser.add_argument('-v', '--verbosity', dest='verbosity',
                        default=0, action='count',
                        help='Increase output verbosity')
    parser.add_argument('--timeFormat', default=DEFAULT_TIME_FORMAT,
                        help='timestamp format, default: %(default)')
    parser.add_argument('--startDT', default=None,
                        type=lambda s: datetime.strptime(s, '%Y-%m-%dT%H:%M:%S.%fZ'),
                        help=' crop start timestamp (iso8601)')
    parser.add_argument('--stopDT', default=None,
                        type=lambda s: datetime.strptime(s, '%Y-%m-%dT%H:%M:%S.%fZ'),
                        help=' crop stop timestamp (iso8601)')
    parser.add_argument('dataFile', metavar='dataFile',
                        help='the raw data file to process')

    parsed_args = parser.parse_args()

    ############################
    # Set up logging before we do any other argument parsing (so that we
    # can log problems with argument parsing).
    
    LOGGING_FORMAT = '%(asctime)-15s %(levelname)s - %(message)s'
    logging.basicConfig(format=LOGGING_FORMAT)

    LOG_LEVELS = {0: logging.WARNING, 1: logging.INFO, 2: logging.DEBUG}
    parsed_args.verbosity = min(parsed_args.verbosity, max(LOG_LEVELS))
    logging.getLogger().setLevel(LOG_LEVELS[parsed_args.verbosity])

    ovdm_parser = SBE21TSGParser(start_dt=parsed_args.startDT, stop_dt=parsed_args.stopDT, time_format=parsed_args.timeFormat)

    try:
        logging.info("Processing file: {}".format(parsed_args.dataFile))
        ovdm_parser.process_file(parsed_args.dataFile)
        print(ovdm_parser.toJSON())
        logging.info("Done!")
    except Exception as err:
        logging.error(str(err))
        raise err
        sys.exit(1)